{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd9d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from model_utils import nll_categ_global, nll_gauss_global\n",
    "\n",
    "from Models.EmbeddingMul import EmbeddingMul\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, dataset_obj, args):\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "        # NOTE: for feat_select, (col_name, col_type, feat_size) in enumerate(dataset_obj.feat_info)\n",
    "\n",
    "        self.dataset_obj = dataset_obj\n",
    "        self.args = args\n",
    "\n",
    "        self.size_input = len(dataset_obj.cat_cols)*self.args.embedding_size + len(dataset_obj.num_cols)\n",
    "        self.size_output = len(dataset_obj.cat_cols) + len(dataset_obj.num_cols) # 2*\n",
    "\n",
    "        ## Encoder Params\n",
    "\n",
    "        # define a different embedding matrix for each feature\n",
    "        if (dataset_obj.dataset_type == \"image\") and (not dataset_obj.cat_cols):\n",
    "            self.feat_embedd = nn.ModuleList([])\n",
    "        else:\n",
    "            self.feat_embedd = nn.ModuleList([nn.Embedding(c_size, self.args.embedding_size, max_norm=1)\n",
    "                                             for _, col_type, c_size in dataset_obj.feat_info\n",
    "                                             if col_type==\"categ\"])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.size_input, self.args.layer_size)\n",
    "        self.fc21 = nn.Linear(self.args.layer_size, self.args.latent_dim)\n",
    "        self.fc22 = nn.Linear(self.args.layer_size, self.args.latent_dim)\n",
    "\n",
    "        if args.AVI:\n",
    "            self.qw_fc1 = nn.Linear(self.size_input, self.args.layer_size)\n",
    "            self.qw_fc2 = nn.Linear(self.args.layer_size, len(dataset_obj.feat_info))\n",
    "\n",
    "        ## Decoder Params\n",
    "\n",
    "        self.fc3 = nn.Linear(self.args.latent_dim, self.args.layer_size)\n",
    "\n",
    "        if dataset_obj.dataset_type == \"image\" and (not dataset_obj.cat_cols):\n",
    "            self.out_cat_linears = nn.Linear(self.args.layer_size, self.size_output)\n",
    "        else:\n",
    "            self.out_cat_linears = nn.ModuleList([nn.Linear(self.args.layer_size, c_size) if col_type==\"categ\"\n",
    "                                                 else nn.Linear(self.args.layer_size, c_size) # 2*\n",
    "                                                 for _, col_type, c_size in dataset_obj.feat_info])\n",
    "\n",
    "        ## Log variance of the decoder for real attributes\n",
    "        if dataset_obj.dataset_type == \"image\" and (not dataset_obj.cat_cols):\n",
    "            self.logvar_x = nn.Parameter(torch.zeros(1).float())\n",
    "        else:\n",
    "            if dataset_obj.num_cols:\n",
    "                self.logvar_x = nn.Parameter(torch.zeros(1,len(dataset_obj.num_cols)).float())\n",
    "            else:\n",
    "                self.logvar_x = []\n",
    "\n",
    "        ## Other\n",
    "\n",
    "        if args.activation == 'relu':\n",
    "            self.activ = nn.ReLU()\n",
    "        elif args.activation == 'hardtanh':\n",
    "            self.activ = nn.Hardtanh()\n",
    "\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # define encoder / decoder easy access parameter list\n",
    "        encoder_list = [self.fc1, self.fc21, self.fc22]\n",
    "        self.encoder_mod = nn.ModuleList(encoder_list)\n",
    "        if args.AVI:\n",
    "            encoder_list = [self.qw_fc1, self.qw_fc2]\n",
    "            self.encoder_mod.extend(encoder_list)\n",
    "        if self.feat_embedd:\n",
    "            self.encoder_mod.append(self.feat_embedd)\n",
    "\n",
    "        self.encoder_param_list = nn.ParameterList(self.encoder_mod.parameters())\n",
    "\n",
    "        decoder_list = [self.fc3, self.out_cat_linears]\n",
    "        self.decoder_mod = nn.ModuleList(decoder_list)\n",
    "        self.decoder_param_list = nn.ParameterList(self.decoder_mod.parameters())\n",
    "        if len(self.logvar_x):\n",
    "            self.decoder_param_list.append(self.logvar_x)\n",
    "\n",
    "\n",
    "    def get_inputs(self, x_data, one_hot_categ=False, masking=False, drop_mask=[], in_aux_samples=[]):\n",
    "\n",
    "        \"\"\"\n",
    "            drop_mask: (N,D) defines which entries are to be zeroed-out\n",
    "        \"\"\"\n",
    "\n",
    "        if not masking:\n",
    "            drop_mask = torch.ones(x_data.shape, device=x_data.device)\n",
    "\n",
    "        if not isinstance(in_aux_samples, list):\n",
    "            aux_samples_on = True\n",
    "        else:\n",
    "            aux_samples_on = False\n",
    "\n",
    "        if self.dataset_obj.dataset_type == \"image\" and (not self.dataset_obj.cat_cols):\n",
    "            # image data, hence real\n",
    "            return x_data*drop_mask\n",
    "\n",
    "        else:\n",
    "            # mixed data, or just real or just categ\n",
    "            input_list = []\n",
    "            cursor_embed = 0\n",
    "            start = 0\n",
    "\n",
    "            for feat_idx, ( _, col_type, feat_size ) in enumerate(self.dataset_obj.feat_info):\n",
    "\n",
    "                if one_hot_categ:\n",
    "                    if col_type == \"categ\": # categorical (uses embeddings)\n",
    "                        func_embedd = EmbeddingMul(self.args.embedding_size, x_data.device)\n",
    "                        func_embedd.requires_grad = x_data.requires_grad\n",
    "                        categ_val = func_embedd(x_data[:,start:(start + feat_size)].view(1,x_data.shape[0],-1),\n",
    "                                    self.feat_embedd[cursor_embed].weight,-1, max_norm=1, one_hot_input=True)\n",
    "                        input_list.append(categ_val.view(x_data.shape[0],-1)*drop_mask[:,feat_idx].view(-1,1))\n",
    "\n",
    "                        start += feat_size\n",
    "                        cursor_embed += 1\n",
    "\n",
    "                    elif col_type == \"real\": # numerical\n",
    "                        input_list.append((x_data[:,start]*drop_mask[:,feat_idx]).view(-1,1))\n",
    "                        start += 1\n",
    "\n",
    "                else:\n",
    "                    if col_type == \"categ\": # categorical (uses embeddings)\n",
    "                        if aux_samples_on:\n",
    "                            aux_categ = self.feat_embedd[cursor_embed](x_data[:,feat_idx].long())*drop_mask[:,feat_idx].view(-1,1) \\\n",
    "                                + (1.-drop_mask[:,feat_idx].view(-1,1))*self.feat_embedd[cursor_embed](in_aux_samples[:,feat_idx].long())\n",
    "                        else:\n",
    "                            aux_categ = self.feat_embedd[cursor_embed](x_data[:,feat_idx].long())*drop_mask[:,feat_idx].view(-1,1)\n",
    "                        input_list.append(aux_categ)\n",
    "                        cursor_embed += 1\n",
    "\n",
    "                    elif col_type == \"real\": # numerical\n",
    "                        if aux_samples_on:\n",
    "                            input_list.append((x_data[:,feat_idx]*drop_mask[:,feat_idx]).view(-1,1) \\\n",
    "                                + ((1.-drop_mask[:,feat_idx])*in_aux_samples[:,feat_idx]).view(-1,1) )\n",
    "                        else:\n",
    "                            input_list.append((x_data[:,feat_idx]*drop_mask[:,feat_idx]).view(-1,1))\n",
    "\n",
    "            return torch.cat(input_list, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, x_data, one_hot_categ=False, masking=False, drop_mask=[], in_aux_samples=[]):\n",
    "\n",
    "        q_params = dict()\n",
    "\n",
    "        input_values = self.get_inputs(x_data, one_hot_categ, masking, drop_mask, in_aux_samples)\n",
    "\n",
    "        fc1_out = self.fc1(input_values)\n",
    "\n",
    "        h1_qz = self.activ(fc1_out)\n",
    "\n",
    "        q_params['z'] = {'mu': self.fc21(h1_qz), 'logvar': self.fc22(h1_qz)}\n",
    "\n",
    "        if self.args.AVI:\n",
    "\n",
    "            qw_fc1_out = self.qw_fc1(input_values)\n",
    "\n",
    "            h1_qw = self.activ(qw_fc1_out)\n",
    "\n",
    "            q_params['w'] = {'logit_pi': self.qw_fc2(h1_qw)}\n",
    "\n",
    "        return q_params\n",
    "\n",
    "    def sample_normal(self, q_params_z, eps=None):\n",
    "\n",
    "        if self.training:\n",
    "\n",
    "            if eps is None:\n",
    "                eps = torch.randn_like(q_params_z['mu'])\n",
    "\n",
    "            std = q_params_z['logvar'].mul(0.5).exp_()\n",
    "\n",
    "            return eps.mul(std).add_(q_params_z['mu'])\n",
    "\n",
    "        else:\n",
    "            return q_params_z['mu']\n",
    "\n",
    "    def reparameterize(self, q_params, eps_samples=None):\n",
    "\n",
    "        q_samples = dict()\n",
    "\n",
    "        q_samples['z'] = self.sample_normal(q_params['z'], eps_samples)\n",
    "\n",
    "        return q_samples\n",
    "\n",
    "\n",
    "    def decode(self, z):\n",
    "\n",
    "        p_params = dict()\n",
    "\n",
    "        h3 = self.activ(self.fc3(z))\n",
    "\n",
    "        if self.dataset_obj.dataset_type == 'image' and (not self.dataset_obj.cat_cols):\n",
    "\n",
    "            # tensor with dims (batch_size, self.size_output)\n",
    "            p_params['x'] = self.out_cat_linears(h3)\n",
    "            p_params['logvar_x'] = self.logvar_x.clamp(-3,3)\n",
    "\n",
    "        else:\n",
    "            out_cat_list = []\n",
    "\n",
    "            for feat_idx, out_cat_layer in enumerate(self.out_cat_linears):\n",
    "\n",
    "                if self.dataset_obj.feat_info[feat_idx][1] == \"categ\": # coltype check\n",
    "                    out_cat_list.append(self.logSoftmax(out_cat_layer(h3)))\n",
    "\n",
    "                elif self.dataset_obj.feat_info[feat_idx][1] == \"real\":\n",
    "                    out_cat_list.append(out_cat_layer(h3))\n",
    "\n",
    "            # tensor with dims (batch_size, self.size_output)\n",
    "            p_params['x'] = torch.cat(out_cat_list, 1)\n",
    "\n",
    "            if self.dataset_obj.num_cols:\n",
    "                p_params['logvar_x'] = self.logvar_x.clamp(-3,3)\n",
    "\n",
    "        return p_params\n",
    "\n",
    "\n",
    "    def forward(self, x_data, n_epoch=None, one_hot_categ=False, masking=False, drop_mask=[], in_aux_samples=[]):\n",
    "\n",
    "        q_params = self.encode(x_data, one_hot_categ, masking, drop_mask, in_aux_samples)\n",
    "        q_samples = self.reparameterize(q_params)\n",
    "\n",
    "        return self.decode(q_samples['z']), q_params, q_samples\n",
    "\n",
    "\n",
    "    def loss_function(self, input_data, p_params, q_params, q_samples, clean_comp_only=False, data_eval_clean=False):\n",
    "\n",
    "        \"\"\" ELBO: reconstruction loss for each variable + KL div losses summed over elements of a batch \"\"\"\n",
    "\n",
    "        dtype_float = torch.cuda.FloatTensor if self.args.cuda_on else torch.FloatTensor\n",
    "        nll_val = torch.zeros(1).type(dtype_float)\n",
    "\n",
    "        if self.dataset_obj.dataset_type == 'image' and (not self.dataset_obj.cat_cols):\n",
    "            # image datasets, large number of features (so vectorize loss and pi calc.)\n",
    "            pi_feat = torch.sigmoid(q_params['w']['logit_pi']).clamp(1e-6, 1-1e-6)\n",
    "\n",
    "            if clean_comp_only and data_eval_clean:\n",
    "                pi_feat = torch.ones_like(q_params['w']['logit_pi'])\n",
    "\n",
    "            nll_val = nll_gauss_global(p_params['x'],\n",
    "                                       input_data,\n",
    "                                       p_params['logvar_x'], isRobust=True,\n",
    "                                       std_0_scale=self.args.std_gauss_nll,\n",
    "                                       w=pi_feat, isClean=clean_comp_only,\n",
    "                                       shape_feats=[len(self.dataset_obj.num_cols)]).sum()\n",
    "\n",
    "        else:\n",
    "            # mixed datasets, or just categorical / continuous with medium number of features\n",
    "            start = 0\n",
    "            cursor_num_feat = 0\n",
    "\n",
    "            for feat_select, (_, col_type, feat_size) in enumerate(self.dataset_obj.feat_info):\n",
    "\n",
    "\n",
    "                pi_feat = torch.sigmoid(q_params['w']['logit_pi'][:,feat_select]).clamp(1e-6, 1-1e-6)\n",
    "\n",
    "                if clean_comp_only and data_eval_clean:\n",
    "                    pi_feat = torch.ones_like(q_params['w']['logit_pi'][:,feat_select])\n",
    "\n",
    "                # compute NLL\n",
    "                if col_type == 'categ':\n",
    "\n",
    "                    nll_val += nll_categ_global(p_params['x'][:,start:(start + feat_size)],\n",
    "                                                input_data[:,feat_select].long(), feat_size, isRobust=True,\n",
    "                                                w=pi_feat, isClean=clean_comp_only).sum()\n",
    "\n",
    "                    start += feat_size\n",
    "\n",
    "                elif col_type == 'real':\n",
    "\n",
    "                    nll_val += nll_gauss_global(p_params['x'][:,start:(start + 1)], # 2\n",
    "                                                input_data[:,feat_select],\n",
    "                                                p_params['logvar_x'][:,cursor_num_feat], isRobust=True,\n",
    "                                                w=pi_feat, isClean=clean_comp_only, \n",
    "                                                std_0_scale=self.args.std_gauss_nll).sum()\n",
    "\n",
    "                    start += 1 # 2\n",
    "                    cursor_num_feat +=1\n",
    "\n",
    "\n",
    "        # kld regularizer on the latent space\n",
    "        z_kld = -0.5 * torch.sum(1 + q_params['z']['logvar'] - q_params['z']['mu'].pow(2) - q_params['z']['logvar'].exp())\n",
    "\n",
    "        \n",
    "        \n",
    "        # prior on clean cells (higher values means more likely to be clean)\n",
    "        prior_sig = torch.tensor(self.args.alpha_prior).type(dtype_float)\n",
    "\n",
    "        # kld regularized on the weights\n",
    "        pi_mtx = torch.sigmoid(q_params['w']['logit_pi']).clamp(1e-6, 1-1e-6)\n",
    "        w_kld = torch.sum(pi_mtx * torch.log(pi_mtx / prior_sig) + (1-pi_mtx) * torch.log((1-pi_mtx) / (1-prior_sig)))\n",
    "\n",
    "        loss_ret = nll_val + z_kld if clean_comp_only else nll_val + z_kld + w_kld\n",
    "\n",
    "        return loss_ret, nll_val, z_kld, w_kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7047a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('tf_env1.': conda)",
   "language": "python",
   "name": "python3810jvsc74a57bd0d62a3a3ce03ba98fc859896d91ce0003b1eb7a8bae8ba053e42c1ba4fa0101c8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
